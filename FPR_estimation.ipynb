{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb926a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from load_fdb_datasets import prepare_noisy_dataset, dataset_stats\n",
    "from cleanlab.filter import find_label_issues\n",
    "from catboost import CatBoostClassifier\n",
    "from micro_models import MicroModelEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27f5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some auxilliary functions\n",
    "\n",
    "# given a model, a validation set, and an FPR target, \n",
    "# find a score threshold that achieves that FPR\n",
    "def validate(clf, X, y, fpr_target):\n",
    "    preds = clf.predict_proba(X)[:,1]\n",
    "    fpr, tpr, thresh = roc_curve(y, preds)\n",
    "    idx = max(np.where(fpr < fpr_target)[0])\n",
    "    return thresh[idx]\n",
    "\n",
    "# given a model, a test set, and a score threshold,\n",
    "# find the FPR/TPR at that threshold\n",
    "def evaluate(clf, X, y, threshold):\n",
    "    preds = clf.predict_proba(X)[:,1]\n",
    "    fpr, tpr, thresh = roc_curve(y, preds)\n",
    "    idx = min(np.where(thresh < threshold)[0])\n",
    "    fpr = fpr[idx]\n",
    "    tpr = tpr[idx]\n",
    "    acc = accuracy_score(y, 1.0*(preds > threshold))\n",
    "    \n",
    "    return fpr, tpr, acc\n",
    "\n",
    "# given a model, a validation set, a test set, and an FPR target\n",
    "# first validate the model, then test to see actual performance\n",
    "def validate_and_evaluate(clf, X_vl, y_vl, X_ts, y_ts, fpr_target):\n",
    "    thresh = validate(clf, X_vl, y_vl, fpr_target)\n",
    "    fpr, tpr, acc = evaluate(clf, X_ts, y_ts, thresh)\n",
    "    print(f\"fpr: {fpr:.3f}, tpr: {tpr:.3f}, acc: {acc:.3f} at thresh {thresh:.3f}\")\n",
    "    return fpr\n",
    "\n",
    "# use a micro-model ensemble to clean a validation set\n",
    "# by removing (up to) the 'num' noisiest examples  \n",
    "def mm_clean(mm_cleaner, X_vl, y_vl, num):\n",
    "    anomaly_df = X_vl.copy()\n",
    "    clean_preds = mm_cleaner.predict_proba(X_vl)\n",
    "    anomaly_df['clean_preds'] = clean_preds\n",
    "    potential_noise = anomaly_df[(y_vl == 0) & (anomaly_df.clean_preds > 0)]\n",
    "    noise_idx = potential_noise.sort_values(by='clean_preds', ascending=False)[:num].index\n",
    "    idx = ~X_vl.index.isin(noise_idx)\n",
    "    print(f\"{X_vl.shape[0] - X_vl[idx].shape[0]} examples filtered\")\n",
    "    return X_vl[idx].reset_index(drop=True), y_vl[idx], idx\n",
    "\n",
    "# use a trained model to directly clean a validation set\n",
    "# by removing the 'num' noisiest examples \n",
    "def direct_clean(clf, X_vl, y_vl, num):\n",
    "    anomaly_df = X_vl.copy()\n",
    "    anomaly_df['clean_preds'] = clf.predict_proba(X_vl)[:,1]\n",
    "    potential_noise = anomaly_df[(y_vl == 0) & (anomaly_df.clean_preds > 0)]\n",
    "    noise_idx = potential_noise.sort_values(by='clean_preds', ascending=False)[:num].index\n",
    "    idx = ~X_vl.index.isin(noise_idx)\n",
    "    print(f\"{X_vl.shape[0] - X_vl[idx].shape[0]} examples filtered\")\n",
    "    return X_vl[idx].reset_index(drop=True), y_vl[idx], idx\n",
    "\n",
    "# use a trained model with Cleanlab to directly clean a validation set\n",
    "# by removing (up to) the 'num' noisiest examples \n",
    "def cl_clean(clf, X_vl, y_vl, num=0):\n",
    "    is_label_issue = find_label_issues(labels=y_vl, pred_probs=clf.predict_proba(X_vl))\n",
    "    idx = (y_vl == 1) | (~is_label_issue)\n",
    "    return X_vl[idx], y_vl[idx], idx\n",
    "\n",
    "# compute the type-1/type-2 error of a cleaning method\n",
    "def error(valid, idx):\n",
    "    # e_1 - c=0, y=1\n",
    "    #       c=0, y*=0, y=1\n",
    "    #       c=0, y*=1, y=1 doesn't happen\n",
    "    # idx - not removed -> c=0, y*=0 or y*=1\n",
    "    e1 = valid[(idx) & (valid.noise == 1)].shape[0]/valid.shape[0]\n",
    "\n",
    "    # e_2 - c=1, y=0\n",
    "    #       c=1, y*=1, y=0 doesn't happen\n",
    "    #       c=1, y*=0, y=0\n",
    "    # ~idx - removed -> c=1, y*=0\n",
    "    # noise=0 <-> y=0 and y*=0 or y=1 and y*=1\n",
    "    e2 = valid[(~idx) & (valid.noise == 0)].shape[0]/valid.shape[0]\n",
    "    print(f\"type 1 error: {e1:.4f}, type 2 error: {e2:.4f}\")\n",
    "    return e1, e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4d3583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to generate an \"experiment dict\", i.e. a python dict containing a dataset and\n",
    "# all relevant experimental parameters (this will allow us to easily iterate through all \n",
    "# such experiments). This method will add noise to the datasets as they are loaded.\n",
    "# We will also use these dicts to store results of experiments once completed.\n",
    "def generate_experiment(key, fprs):\n",
    "\n",
    "    noise_type = 'time-dependent'\n",
    "    noise_amount = 0.3\n",
    "    split = 0.7\n",
    "    shuffle = True\n",
    "    sort_key = 'creation_date'\n",
    "\n",
    "    dataset = prepare_noisy_dataset(key, noise_type, noise_amount, split=split, shuffle=shuffle, sort_key=sort_key)\n",
    "    dataset_stats(dataset)\n",
    "    \n",
    "    result_dict = {cleaning_type: [] for cleaning_type in ['none','cleanlab','micromodel','direct']}\n",
    "    \n",
    "    experiment = {'description': key,\n",
    "                  'dataset': dataset,\n",
    "                  'cleaned_data': result_dict.copy(),\n",
    "                  'cleaning_error': result_dict.copy(),\n",
    "                  'results': {fpr: result_dict.copy() for fpr in fprs}\n",
    "                 }\n",
    "    \n",
    "    return experiment\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f1de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main experimental process: for each dataset we will:\n",
    "# load the dataset/metadata from FDB and add noise \n",
    "# train a base model and micro-model ensemble\n",
    "# clean the data using each method (none, cleanlab, micro-model, direct)\n",
    "# for each FPR target, determine \"true\" threshold (threshold which achieves FPR on clean data)\n",
    "#   then determine FPR estimate for each method on noisy data and record error \n",
    "\n",
    "datasets = ['cloud', 'ieeecis', 'ccfraud', 'fraudecom', 'sparknov'] \n",
    "fpr_targets = [0.01, 0.02, 0.04, 0.08]\n",
    "\n",
    "experiments = {key: generate_experiment(key, fpr_targets) for key in datasets}\n",
    "\n",
    "for dataset_key in experiments:\n",
    "    \n",
    "    experiment = experiments[dataset_key]\n",
    "    dataset = experiment['dataset']\n",
    "    \n",
    "    # extract features and subsets from main dataset\n",
    "    features, cat_features, label = dataset['features'], dataset['cat_features'], dataset['label']\n",
    "    train, valid, test = dataset['train'], dataset['valid'], dataset['test']\n",
    "    X_train, y_train = train[features], train[label].values.reshape(-1)\n",
    "    X_valid, y_valid = valid[features], valid[label].values.reshape(-1)\n",
    "    \n",
    "    # y_true is true labels for validation data\n",
    "    y_true = y_valid.copy()\n",
    "    y_true[valid.noise==1] = 1\n",
    "    \n",
    "    # train base model \n",
    "    print(f\"training model / cleaner for {dataset_key}\\n\")\n",
    "    model_params = {\n",
    "                'cat_features':cat_features,\n",
    "                'verbose':False,\n",
    "                'iterations':500\n",
    "            }\n",
    "    base_model = CatBoostClassifier(**model_params)\n",
    "    base_model.fit(X_train, y_train)\n",
    "    \n",
    "    # create micro-model cleaning ensemble\n",
    "    mm_cleaner = MicroModelEnsemble(CatBoostClassifier, num_clfs=16, score_type='preds_avg', **model_params)\n",
    "    mm_cleaner.fit(X_train, y_train)\n",
    "            \n",
    "    # analyze total error in untouched noisy validation data\n",
    "    print(f\"cleaning {dataset_key} dataset with 'none'\\n\")\n",
    "    experiment['cleaning_error']['none'] = error(valid,valid.index.isin(valid.index))\n",
    "\n",
    "    # clean dataset using various methods and record t1/t2 error\n",
    "    print(f\"cleaning {dataset_key} dataset with 'cleanlab'\\n\")\n",
    "    X_vl_cl, y_vl_cl, idx = cl_clean(base_model, X_valid, y_valid)\n",
    "    experiment['cleaning_error']['cleanlab'] = error(valid,idx)\n",
    "\n",
    "    print(f\"cleaning {dataset_key} dataset with 'micromodel'\\n\")\n",
    "    X_vl_mm, y_vl_mm, idx = mm_clean(mm_cleaner, X_valid, y_valid, valid.noise.sum())\n",
    "    experiment['cleaning_error']['micromodel'] = error(valid,idx)\n",
    "\n",
    "    print(f\"cleaning {dataset_key} dataset with 'direct'\\n\")\n",
    "    X_vl_dr, y_vl_dr, idx = direct_clean(base_model, X_valid, y_valid, valid.noise.sum())\n",
    "    experiment['cleaning_error']['direct'] = error(valid,idx)\n",
    "\n",
    "    # store cleaned datasets in result dict\n",
    "    experiment['cleaned_data'] = {\n",
    "        'none': (X_valid, y_valid), \n",
    "        'cleanlab': (X_vl_cl, y_vl_cl),\n",
    "        'micromodel': (X_vl_mm, y_vl_mm), \n",
    "        'direct': (X_vl_dr, y_vl_dr)\n",
    "    }\n",
    "    print(f\"doing fpr validation for {dataset_key}\\n\")\n",
    "\n",
    "    for fpr_target in experiment['results'].keys():\n",
    "        print(f\"\\ntarget fpr: {fpr_target:.3f}\")\n",
    "        true_thresh = validate(base_model, X_valid, y_true, fpr_target)\n",
    "\n",
    "        for cleaning_type, (X_clean, y_clean) in experiment['cleaned_data'].items():\n",
    "            fpr, tpr, acc = evaluate(base_model, X_clean, y_clean, true_thresh)\n",
    "            err = np.abs((fpr - fpr_target)/fpr_target)\n",
    "            experiment['results'][fpr_target][cleaning_type] = (fpr, err)\n",
    "            print(f\"{cleaning_type} estimate: {fpr:.3f}, err: {err:.3f}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f6d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to highlight best performance among experiment results\n",
    "\n",
    "def highlight_max(s, props=''):\n",
    "    return np.where(s == np.nanmax(s.values), props, '')\n",
    "\n",
    "def highlight_min(s, props=''):\n",
    "    return np.where(s == np.nanmin(s.values), props, '')\n",
    "\n",
    "def bold_extreme_values(data, data_max=-1):\n",
    "\n",
    "    if data == data_max:\n",
    "        return \"\\textbf{%s}\" % data\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b31aa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load results of experiments into dataframes for examination\n",
    "\n",
    "rows = pd.Index(['None','CleanLab','MicroModel','Direct'])\n",
    "row_key = {'none': 'None',\n",
    "           'cleanlab': 'CleanLab',\n",
    "           'micromodel': 'MicroModel',\n",
    "           'direct': 'Direct'}\n",
    "fpr_cols = pd.MultiIndex.from_product([fpr_targets,['fpr','err']], names=['Target FPR', 'Metric'])\n",
    "fpr_dfs = {}\n",
    "\n",
    "cln_error_cols = pd.MultiIndex.from_product([datasets,['type 1', 'type 2']], names=['dataset', 'Error Type'])\n",
    "\n",
    "cln_error_df = pd.DataFrame(index=rows, columns=cln_error_cols)\n",
    "\n",
    "for dataset_key in experiments:\n",
    "    experiment = experiments[dataset_key]\n",
    "    fpr_df = pd.DataFrame(index=rows, columns=fpr_cols)\n",
    "    \n",
    "    for cleaning_type in experiment['cleaning_error']:\n",
    "        e1, e2 = experiment['cleaning_error'][cleaning_type]\n",
    "        # print(f\"{key} type-1/type-2 cleaning error: {e1:.4f}/ {e2:.4f}\")\n",
    "        cln_error_df.loc[row_key[cleaning_type], (dataset_key,'type 1')] = f\"{e1:.3f}\"\n",
    "        cln_error_df.loc[row_key[cleaning_type], (dataset_key,'type 2')] = f\"{e2:.3f}\"\n",
    "\n",
    "    for fpr_target in experiment['results']:\n",
    "        for cleaning_type in experiment['results'][fpr_target]:\n",
    "            fpr, err = experiment['results'][fpr_target][cleaning_type]\n",
    "            fpr_df.loc[row_key[cleaning_type], (fpr_target, 'fpr')] = f\"{fpr:.3f}\"\n",
    "            fpr_df.loc[row_key[cleaning_type], (fpr_target, 'err')] = f\"{err:.2f}\"\n",
    "            \n",
    "    fpr_dfs[dataset_key] = fpr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b79996",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display results in dataframes\n",
    "\n",
    "display(cln_error_df) #.style.apply(highlight_min, props='font-weight:bold;background-color:lightblue', axis=0, \n",
    "                      #     subset=[[d,['type 1','type 2']] for d in datasets]))\n",
    "\n",
    "for experiment_key in experiments:\n",
    "    fpr_df = fpr_dfs[experiment_key]\n",
    "    \n",
    "    print(f\"\\n={experiment_key}=\\n\")\n",
    "    \n",
    "    display(fpr_df.style.apply(highlight_min, props='font-weight:bold;background-color:lightblue', axis=0, \n",
    "                           subset=[[t,'err'] for t in fpr_targets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b12213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export type-1/type-2 error results to LaTeX\n",
    "\n",
    "s = '\\\\begin{table}[!htbp]\\n'\n",
    "s = s + '\\\\caption {Cleaning Errors}\\n'\n",
    "s = s + '\\label{tab:cleaning_error}\\n'\n",
    "s = s + cln_error_df.to_latex(escape=False) + '\\n'\n",
    "s = s + '\\end{table}'        \n",
    "print(s.replace('l'*(2*len(datasets)+1),('l'+'ll|'*(len(datasets)))[:-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bb5414",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# export FPR estimation error results to LaTeX\n",
    "\n",
    "for dataset_key in experiments:\n",
    "    fpr_df = fpr_dfs[dataset_key]\n",
    "    columns = [(t,'err') for t in fpr_targets]\n",
    "    for col in columns:\n",
    "        fpr_df[col] = fpr_df[col].apply(lambda data : bold_extreme_values(data, data_max=fpr_df[col].min()))\n",
    "    s = '\\\\begin{table}[!htbp]\\n'\n",
    "    s = s + '\\\\caption {{Results for Dataset \\\\texttt{{{}}}}}\\n'.format(dataset_key)\n",
    "    s = s + '\\label{{tab:exact_{}}}\\n'.format(dataset_key)\n",
    "    s = s + fpr_df.to_latex(escape=False) + '\\n'\n",
    "    s = s + '\\end{table}\\n'        \n",
    "    print(s.replace('lllllllll','lll|ll|ll|ll'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372a1e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ec1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
